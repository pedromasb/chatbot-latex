{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VHNllKuGKuFo",
        "gJ44CCR5Lr2n",
        "PJxNwegfL0db"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "collapsed": true,
        "id": "AVEj2DOaLAzh",
        "outputId": "0cb151ea-d2a0-4be1-eaba-a520b52bc342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pinecone\n",
            "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2025.8.3)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
            "Collecting packaging<25.0,>=24.2 (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
            "Downloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pinecone-plugin-interface, packaging, pinecone-plugin-assistant, pinecone\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "Successfully installed packaging-24.2 pinecone-7.3.0 pinecone-plugin-assistant-1.7.0 pinecone-plugin-interface-0.0.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              },
              "id": "b8114448a6084234ac2485ad36fc48ba"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chiUa9-nZYJZ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "import math\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading chunks"
      ],
      "metadata": {
        "id": "VHNllKuGKuFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_path = \"/content/chunks.jsonl\"\n",
        "chunks = []\n",
        "with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        chunks.append(json.loads(line))"
      ],
      "metadata": {
        "id": "u5kYT3Jka0Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[120]"
      ],
      "metadata": {
        "id": "xm_M_Og3aO0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74fd552c-54e4-4e7c-d01a-9104e1dcb312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'ec8ff485-f8e7-4997-b7ff-5c4977cd622d',\n",
              " 'text': '(CITATION) and (CITATION) . Our values are in average higher than the aforementioned relations during the L/T transition. Since (CITATION) uses also uses a sample of ultracool dwarfs extracted from the UltracoolSheet catalogue, we can directly compare our effective temperature determinations with their semi-empirical values. Figure illustrates this comparison, confirming a good consistency between the two sets and a deviation towards higher values in our temperatures for the L/T transition. This transition is still a less understood phase of ultracool dwarf evolution. The increase of cloud opacity from early-L to late-L dwarfs, and the evolution to cloudless T dwarfs, hugely complicates the modelling of these atmospheres. In the future, a better treatment of clouds for this transition in atmospheric models will be the key to mitigating this effect. The results obtained in this study indicate that the methodology presented by (CITATION) , developed for the determination of stellar parameters of M dwarfs from high-resolution spectra, can be successfully adapted to the low-resolution domain to estimate the effective temperature of ultracool dwarfs. In this line, the methodology consolidated in this chapter will serve as a basis for the characterisation of ultracool dwarfs in the promising surveys to come in the next years, which envisage a scientific leap in this field, starting with its direct application to the wide-field Euclid low-resolution spectroscopic survey. We are already making progress in this regard, working with the first spectroscopic data from Euclid, and have successfully tailored the procedure to its wavelength solution. Doing this, we have applied the methodology to near-infrared, low-resolution Euclid spectra of a sample of confirmed ultracool dwarfs (see Figure ), and determined the effective temperatures of these objects, which are in excellent agreement with the spectral types derived by comparing them to the standard templates published by SPLAT (see Dominguez-Tagle et',\n",
              " 'type': 'body',\n",
              " 'page': -1,\n",
              " 'chapter_key': '5',\n",
              " 'chapter': 'Characterisation of Ultracool Dwarfs with Deep Transfer Learning',\n",
              " 'section_key': '5.2',\n",
              " 'section': 'Ultracool dwarf characterisation',\n",
              " 'subsection_key': None,\n",
              " 'subsection': None,\n",
              " 'thesis_part': 'Methods/Results',\n",
              " 'chunk_idx': 2,\n",
              " 'chunk_total': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PINECONE"
      ],
      "metadata": {
        "id": "BqxBaJ9sy_NN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Pinecone index"
      ],
      "metadata": {
        "id": "gJ44CCR5Lr2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pc = Pinecone(api_key=pinecone_key)\n",
        "\n",
        "index_name = \"thesis-chat\"\n",
        "dimension = 768  # all-mpnet-base-v2\n",
        "metric = \"cosine\"\n",
        "\n",
        "# Create index if it doesn't exist\n",
        "if index_name not in [idx.name for idx in pc.list_indexes()]:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=dimension,\n",
        "        metric=metric,                  # \"cosine\" | \"dotproduct\" | \"euclidean\"\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",                # or \"gcp\", check your account availability\n",
        "            region=\"us-east-1\"          # pick a region close to your server\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "iHlEOMvcy_d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc.list_indexes()[0]"
      ],
      "metadata": {
        "id": "muupHQ8iy_g3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180aff9d-16bd-4dc9-de15-038145799207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              "    \"name\": \"thesis-chat\",\n",
              "    \"metric\": \"cosine\",\n",
              "    \"host\": \"thesis-chat-gs4mxea.svc.aped-4627-b74a.pinecone.io\",\n",
              "    \"spec\": {\n",
              "        \"serverless\": {\n",
              "            \"cloud\": \"aws\",\n",
              "            \"region\": \"us-east-1\"\n",
              "        }\n",
              "    },\n",
              "    \"status\": {\n",
              "        \"ready\": true,\n",
              "        \"state\": \"Ready\"\n",
              "    },\n",
              "    \"vector_type\": \"dense\",\n",
              "    \"dimension\": 768,\n",
              "    \"deletion_protection\": \"enabled\",\n",
              "    \"tags\": null\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2RWufUwKEv7",
        "outputId": "b5296fb7-3e30-49df-f062-2d83aa8b1154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 768,\n",
              " 'index_fullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {'v1': {'vector_count': 130}},\n",
              " 'total_vector_count': 130,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upserting Embeddings in Pinecone index"
      ],
      "metadata": {
        "id": "PJxNwegfL0db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- config ---\n",
        "PINECONE_API_KEY = pinecone_key  # you already defined this\n",
        "INDEX_NAME = \"thesis-chat\"\n",
        "NAMESPACE = \"multiling\"                 # change if you want versioning\n",
        "BATCH_SIZE = 200                 # Pinecone likes 100–500; tune as needed\n",
        "EMBED_FILE = \"/content/chunks_with_embeddings_multiling.jsonl\"  # produced earlier\n",
        "EXPECTED_DIM = 768               # all-mpnet-base-v2\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(INDEX_NAME)\n",
        "\n",
        "def iter_jsonl(path: str):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                yield json.loads(line)\n",
        "\n",
        "# (Optional) wipe the namespace first if you want a clean slate\n",
        "# index.delete(delete_all=True, namespace=NAMESPACE)"
      ],
      "metadata": {
        "id": "TOPxFxu0L0-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- helpers to sanitize metadata ---\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "def _clean_value(v: Any):\n",
        "    # Drop None entirely\n",
        "    if v is None:\n",
        "        return None\n",
        "    # Allowed scalars\n",
        "    if isinstance(v, (str, int, float, bool)):\n",
        "        return v\n",
        "    # Lists must be list of strings\n",
        "    if isinstance(v, list):\n",
        "        out = [str(x) for x in v if x is not None]\n",
        "        return out\n",
        "    # Fallback: stringify (e.g., dicts)\n",
        "    return str(v)\n",
        "\n",
        "def sanitize_meta(meta: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    cleaned = {}\n",
        "    for k, v in meta.items():\n",
        "        cv = _clean_value(v)\n",
        "        if cv is not None:\n",
        "            cleaned[k] = cv\n",
        "    return cleaned\n",
        "\n",
        "# Optional: keep text metadata bounded (Pinecone metadata should be small)\n",
        "MAX_TEXT_CHARS = 4000  # adjust if you like\n",
        "\n",
        "def clip_text(s: str) -> str:\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = str(s)\n",
        "    return s if len(s) <= MAX_TEXT_CHARS else s[:MAX_TEXT_CHARS]\n",
        "\n",
        "# --- batching / flush with sanitization + better error output ---\n",
        "buffer = []\n",
        "n = 0\n",
        "\n",
        "def flush():\n",
        "    global buffer, n\n",
        "    if not buffer:\n",
        "        return\n",
        "    try:\n",
        "        index.upsert(vectors=buffer, namespace=NAMESPACE)\n",
        "    except Exception as e:\n",
        "        # Try to pinpoint the first offending record\n",
        "        print(\"Upsert failed; inspecting batch...\")\n",
        "        for rec in buffer:\n",
        "            bad = {}\n",
        "            for k, v in rec.get(\"metadata\", {}).items():\n",
        "                if v is None:\n",
        "                    bad[k] = v\n",
        "            if bad:\n",
        "                print(\"Found None metadata fields in record id:\", rec.get(\"id\"), \"->\", bad)\n",
        "                break\n",
        "        raise  # re-raise after printing details\n",
        "    n += len(buffer)\n",
        "    print(f\"Upserted {n} vectors in total...\")\n",
        "    buffer = []\n",
        "\n",
        "# --- main loop (reads from your JSONL with embeddings) ---\n",
        "for row in iter_jsonl(EMBED_FILE):\n",
        "    emb = row.get(\"embedding\")\n",
        "    if not emb:\n",
        "        raise ValueError(\"Row has no 'embedding'. Did you run the embedding step and save chunks_with_embeddings.jsonl?\")\n",
        "    if len(emb) != EXPECTED_DIM:\n",
        "        raise ValueError(f\"Embedding dim mismatch: got {len(emb)}, expected {EXPECTED_DIM}\")\n",
        "\n",
        "    vid = row.get(\"id\") or f\"chunk-{n}\"\n",
        "\n",
        "    raw_meta = {\n",
        "        \"text\": clip_text(row.get(\"text\", \"\")),\n",
        "        \"type\": row.get(\"type\"),\n",
        "        \"chapter_key\": row.get(\"chapter_key\"),\n",
        "        \"chapter\": row.get(\"chapter\"),\n",
        "        \"section_key\": row.get(\"section_key\"),\n",
        "        \"section\": row.get(\"section\"),\n",
        "        \"subsection_key\": row.get(\"subsection_key\"),\n",
        "        \"subsection\": row.get(\"subsection\"),\n",
        "        \"thesis_part\": row.get(\"thesis_part\"),\n",
        "    }\n",
        "    meta = sanitize_meta(raw_meta)\n",
        "\n",
        "    buffer.append({\n",
        "        \"id\": vid,\n",
        "        \"values\": emb,      # list[float], length 768\n",
        "        \"metadata\": meta,   # cleaned\n",
        "    })\n",
        "\n",
        "    if len(buffer) >= BATCH_SIZE:\n",
        "        flush()\n",
        "\n",
        "flush()  # send the final partial batch\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpnOkNfEL1NG",
        "outputId": "2d39efa3-0d94-4232-9f1d-f6177dab1f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 130 vectors in total...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.describe_index_stats()"
      ],
      "metadata": {
        "id": "ZD2SMYsWy_jT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a378736d-0eae-4696-d288-36f7d058ac93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 768,\n",
              " 'index_fullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {'multiling': {'vector_count': 130},\n",
              "                'v1': {'vector_count': 130}},\n",
              " 'total_vector_count': 260,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Test"
      ],
      "metadata": {
        "id": "7vlCf8FSktnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- config ---\n",
        "PINECONE_API_KEY = pinecone_key  # you already defined this\n",
        "INDEX_NAME = \"thesis-chat\"\n",
        "NAMESPACE = \"multiling\"                 # change if you want versioning\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(INDEX_NAME)\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")  # fast & good\n",
        "\n",
        "query = \"Cuáles son las mayores dificutlades a la hora de estimar la temperatura efectiva de las enanas ultrafrías?\"\n",
        "qvec = model.encode([query], convert_to_numpy=True)[0].tolist()\n",
        "\n",
        "res = index.query(\n",
        "    vector=qvec,\n",
        "    top_k=50,\n",
        "    include_metadata=True,\n",
        "    namespace=NAMESPACE\n",
        ")\n",
        "\n",
        "pairs = [(query, m[\"metadata\"].get(\"text\",\"\")) for m in res[\"matches\"]]\n",
        "scores = reranker.predict(pairs)\n",
        "order = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
        "\n",
        "top = [res[\"matches\"][i] for i in order[:10]]\n",
        "# for i, m in enumerate(top, 1):\n",
        "#     print(f\"#{i}  score={scores[order[i-1]]:.3f}  ch={m['metadata'].get('chapter')} | sec={m['metadata'].get('section')} | type={m['metadata'].get('type')} .\")\n",
        "#     print(m[\"metadata\"].get(\"text\",\"\")[:400], \"…\\n\")"
      ],
      "metadata": {
        "id": "JSLtwOFSr3MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Build prompt from top passages and ask an LLM ===\n",
        "\n",
        "# 1) Prepare numbered context blocks (dedupe + trim)\n",
        "def as_path(md):\n",
        "    parts = []\n",
        "    if md.get(\"chapter_key\") or md.get(\"chapter\"):\n",
        "        parts.append(f\"Ch.{md.get('chapter_key') or ''}: {md.get('chapter') or ''}\".strip())\n",
        "    if md.get(\"section_key\") or md.get(\"section\"):\n",
        "        parts.append(f\"S.{md.get('section_key') or ''}: {md.get('section') or ''}\".strip())\n",
        "    if md.get(\"subsection_key\") or md.get(\"subsection\"):\n",
        "        parts.append(f\"SS.{md.get('subsection_key') or ''}: {md.get('subsection') or ''}\".strip())\n",
        "    if md.get(\"type\"):\n",
        "        parts.append(f\"Text type: {md.get('type') or ''}\".strip())\n",
        "    return \" | \".join([p for p in parts if p and not p.endswith(':')]).strip()\n",
        "\n",
        "def trim(s: str, max_chars=1200):\n",
        "    s = (s or \"\").strip()\n",
        "    return s if len(s) <= max_chars else s[:max_chars] + \" …\"\n",
        "\n",
        "# Deduplicate by vector id (if present)\n",
        "seen = set()\n",
        "contexts = []\n",
        "for m in top:\n",
        "    mid = m.get(\"id\") or m.get(\"vector\", {}).get(\"id\") or id(m)\n",
        "    if mid in seen:\n",
        "        continue\n",
        "    seen.add(mid)\n",
        "    md = m.get(\"metadata\", {}) or {}\n",
        "    contexts.append({\n",
        "        \"text\": trim(md.get(\"text\",\"\")),\n",
        "        \"path\": as_path(md),\n",
        "        \"score\": m.get(\"score\", 0.0)\n",
        "    })\n",
        "\n",
        "# Keep the best K for generation\n",
        "K = 6\n",
        "contexts = contexts[:K]\n",
        "\n",
        "numbered_blocks = []\n",
        "for i, c in enumerate(contexts, 1):\n",
        "    header = f\"[[{i}]] {c['path']}\" if c[\"path\"] else f\"[[{i}]]\"\n",
        "    numbered_blocks.append(f\"{header}\\n{c['text']}\")\n",
        "\n",
        "context_blob = \"\\n\\n---\\n\\n\".join(numbered_blocks)\n",
        "\n",
        "# 2) Build the prompt\n",
        "system_msg = (\n",
        "    \"You answer questions using ONLY the provided context blocks.\"\n",
        "    \"You answer questions in a extended way, don't be concise.\"\n",
        "    \"Cite the blocks you use by their bracket number like [1], [2]. \"\n",
        "    \"If the answer is not contained in the context, say you don't know.\"\n",
        ")\n",
        "user_msg = f\"Question: {query}\\n\\nContext:\\n{context_blob}\\n\\n\"\n",
        "\n",
        "# 3) Call the LLM (OpenAI; you can swap in any provider)\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "try:\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-5-mini\",   # or \"gpt-4o\", \"gpt-4.1-mini\", etc.\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\":system_msg},\n",
        "            {\"role\":\"user\",\"content\":user_msg}\n",
        "        ]\n",
        "    )\n",
        "    answer = completion.choices[0].message.content\n",
        "\n",
        "except Exception as e:\n",
        "    answer = f\"(LLM call failed: {e})\"\n",
        "\n",
        "print(\"=== Your question ===\\n\")\n",
        "print(query)\n",
        "print(\"\\n=== Answer ===\\n\")\n",
        "print(answer)\n",
        "print(\"\\n=== Sources used ===\")\n",
        "for i, c in enumerate(contexts, 1):\n",
        "    print(f\"[{i}] {c['path'] or '(no path)'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49QfhjEykt2S",
        "outputId": "0580f451-d001-484a-8d8d-217f96b9cd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Your question ===\n",
            "\n",
            "Cuáles son las mayores dificutlades a la hora de estimar la temperatura efectiva de las enanas ultrafrías?\n",
            "\n",
            "=== Answer ===\n",
            "\n",
            "A continuación describo, basándome en los fragmentos proporcionados, las principales dificultades a la hora de estimar la temperatura efectiva (Teff) de las enanas ultrafrías, con explicaciones detalladas y las referencias a los bloques de contexto usados.\n",
            "\n",
            "1) Física atmosférica compleja — nubes y transición L/T\n",
            "- La fase de transición L→T es “menos entendida” y complica mucho el modelado: el aumento de opacidad por nubes en las L tempranas y la evolución hacia enanas T sin nubes “complica enormemente” el modelado de estas atmósferas. Esto provoca desviaciones en las determinaciones de Teff en esa región y requiere una mejor descripción de las nubes en los modelos para mitigar el problema [2].  \n",
            "\n",
            "2) Espectros dominados por fuertes bandas moleculares\n",
            "- Las enanas ultrafrías, con Teff muy bajas, tienen espectros dominados por fuertes bandas de absorción molecular, lo que dificulta el ajuste espectral y la interpretación de características sensibles a la temperatura (los modelos deben reproducir complejos conjuntos moleculares) [4].\n",
            "\n",
            "3) Limitaciones de los modelos y resolución discreta de la rejilla\n",
            "- Al usar colecciones teóricas como BT‑Settl (CIFIST) en ajustes SED, la Teff estimada queda “discretizada” por el paso de la rejilla de modelos (en el ejemplo citado, pasos de 100 K). Esa discretización limita la precisión con que se puede reportar Teff y puede introducir saltos sistemáticos entre valores contiguos de la rejilla [5].\n",
            "\n",
            "4) Suposiciones fijas en parámetros (log g, metalicidad) durante el ajuste\n",
            "- En el ajuste con VOSA se asumió un rango de log g (4.5–5.5) y metalicidad solar. Estas restricciones implican que variaciones reales de gravedad superficial o metalicidad en los objetos no se exploran completamente durante el ajuste, lo que puede sesgar o aumentar la incertidumbre en la Teff derivada [5].\n",
            "\n",
            "5) Calidad y alcance de los datos observacionales (fotometría / límite de detección)\n",
            "- La construcción del SED depende de múltiples encuestas (J‑PLUS, 2MASS, UKIDSS, WISE, VISTA, SDSS). Además, el límite de magnitud de la encuesta (ej. z ≈ 20.5 AB para J‑PLUS DR2) condiciona la profundidad y la precisión fotométrica disponible; SEDs incompletos o fotometría ruidosa degradan la estimación de Teff [5].\n",
            "\n",
            "6) Contaminación y falsos positivos en la selección previa\n",
            "- Un problema metodológico importante es el alto número de falsos positivos antes de la determinación de Teff (es decir, contaminación en la muestra candidatos), lo que complica y penaliza la fiabilidad estadística de las estimaciones de temperatura hasta depurar la muestra adecuadamente [3].\n",
            "\n",
            "7) Efectos astrofísicos añadidos (rotación rápida y otros) que generan valores atípicos\n",
            "- En trabajos de parámetros estelares se observó una región de outliers a las temperaturas más bajas, poblada mayoritariamente por objetos con alta velocidad de rotación proyectada. Esos rotadores rápidos aparecen como valores atípicos y pueden afectar la determinación y la interpretación de Teff en los conjuntos de datos [6].\n",
            "\n",
            "Conclusión (según el contexto)\n",
            "- En resumen, la estimación de Teff para enanas ultrafrías se ve limitada tanto por complejidades físicas intrínsecas de sus atmósferas (nubes, moléculas, transición L/T) como por limitaciones prácticas en los modelos, en la rejilla de parámetros, en la calidad de los datos y en la selección de muestra. El texto destaca explícitamente que una mejor tratado de las nubes en los modelos será clave para mitigar uno de estos problemas (la L/T transition) [2], y que la discretización y las suposiciones en los ajustes afectan la precisión alcanzable [5].  \n",
            "\n",
            "Si desea, puedo extraer ejemplos concretos de cómo esos efectos se manifiestan en comparaciones entre métodos o proponer qué observables del conjunto de datos citados son más sensibles a cada dificultad; indíqueme cuál de las dificultades quiere explorar con más detalle.\n",
            "\n",
            "=== Sources used ===\n",
            "[1] Ch.4: Autoencoders and Deep Transfer Learning in CARMENES | S.4.1: Context | Text type: body\n",
            "[2] Ch.5: Characterisation of Ultracool Dwarfs with Deep Transfer Learning | S.5.2: Ultracool dwarf characterisation | Text type: body\n",
            "[3] Ch.6: General conclusions and future work | S.6.1: Summary of the Thesis | Text type: body\n",
            "[4] Ch.1: General Introduction | S.1.1: M dwarfs and the substellar realm | SS.1.1.2: ... and beyond | Text type: body\n",
            "[5] Ch.2: Ultracool Dwarfs in J-PLUS | S.2.2: Methodology | SS.2.2.4: VOSA filtering | Text type: body\n",
            "[6] Ch.4: Autoencoders and Deep Transfer Learning in CARMENES | S.4.4: Results and discussion | SS.4.4.1: Stellar parameters analysis | Text type: body\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ch_in_emb: Main challenges (from the provided context):\n",
        "\n",
        "- Spectra dominated by strong molecular absorption and alkali lines, which complicates identification and modelling of temperature-sensitive features (TiO, VO disappearance; strong H2O, metal hydrides; growing Na I and K I lines). This makes spectral fitting and Teff diagnostics intrinsically complex. [3]\n",
        "\n",
        "- The L/T transition produces a narrow range (a “plateau”) in effective temperature: Teff changes very little across the spectral/color transition, so spectral type or colour can be a poor Teff discriminator in that regime. [1]\n",
        "\n",
        "- The “synthetic gap”: differences between synthetic model spectra and observed data hinder direct application of models for Teff estimation, requiring specialized methods (e.g., transfer learning) to bridge the gap. [5]\n",
        "\n",
        "- Strong molecular absorbers (e.g., CH4 in T dwarfs) alter relative band fluxes (H and K suppressed versus J), causing non-monotonic colour–Teff behaviour and complicating photometric Teff estimates. [3][1]\n",
        "\n",
        "- Instrumental/resolution and survey-specific effects (need to adapt methods developed for high-resolution spectra to low-resolution, wide-field surveys) introduce domain-specific challenges that must be accounted for in the Teff determination pipeline. [2]\n",
        "\n",
        "If you want, I can expand on any of these points or list methods used to mitigate each challenge (based on the same context)."
      ],
      "metadata": {
        "id": "yrao0T6QxuOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "v1: The main challenges, as discussed in the provided material, are:\n",
        "\n",
        "- Complex, molecular-dominated spectra: ultracool dwarf spectra are dominated by strong molecular absorption bands (H2O, CH4, metal hydrides) and evolving atomic lines (e.g., Na, K). These strong and temperature-dependent molecular features make it hard to isolate simple, monotonic spectral diagnostics of effective temperature. The spectral appearance changes substantially across M → L → T types (disappearance of TiO/VO, appearance/strengthening of H2O and CH4), complicating Teff inference from spectra or colours alone [3].\n",
        "\n",
        "- The L/T transition temperature degeneracy: across the L → T transition the effective temperature evolves very slowly (a near-constant Teff “plateau”), while colours and spectral appearance change markedly. That narrow Teff range during the L/T transition produces degeneracies and makes Teff estimation particularly uncertain in that regime [1].\n",
        "\n",
        "- Mismatch between synthetic and observed data (\"synthetic gap\"): differences between model (synthetic) spectra and real observed spectra limit the reliability of direct model fitting for Teff. Bridging this synthetic–observed gap is a key difficulty that motivates transfer-learning or empirical approaches [5].\n",
        "\n",
        "- Limited information in low-resolution data: many wide surveys produce low-resolution near‑IR spectra or only photometry, which carry less detailed spectral information and require adapting high-resolution parameter‑estimation methods or developing specialized low‑resolution techniques to recover Teff reliably [2], [5].\n",
        "\n",
        "- Need for multi-dataset/ancillary information and careful selection: robust Teff estimation often requires combining spectra with multi-band photometry, parallaxes, proper motions, or comparison to empirical templates/catalogues (i.e., assembling complementary data and applying tools/ML pipelines), adding practical complexity to the estimation process [6].\n",
        "\n",
        "If you want, I can summarise how each challenge is addressed in the referenced work (methods used to mitigate them) with the same citations."
      ],
      "metadata": {
        "id": "JSe_ddu4yttU"
      }
    }
  ]
}