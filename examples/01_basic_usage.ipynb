{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ThesisChat - Basic Usage Example\n",
    "\n",
    "This notebook demonstrates the basic usage of the ThesisChat module for processing LaTeX documents and creating a conversational interface.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, make sure you have:\n",
    "1. Installed the thesis_chat package\n",
    "2. Obtained API keys for Pinecone and OpenAI\n",
    "3. A LaTeX thesis document to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from thesis_chat import ThesisChat, Config\n",
    "\n",
    "# Set your API keys (preferably as environment variables)\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY', 'your-pinecone-api-key')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your-openai-api-key')\n",
    "\n",
    "# Path to your LaTeX thesis file\n",
    "LATEX_FILE_PATH = \"path/to/your/thesis.tex\"\n",
    "\n",
    "print(\"API keys and file path configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize ThesisChat\n",
    "\n",
    "Create a ThesisChat instance with your configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create custom configuration\n",
    "config = Config(\n",
    "    chunk_size=300,        # Words per chunk\n",
    "    overlap=60,            # Word overlap between chunks\n",
    "    keep_captions=True,    # Include figure/table captions\n",
    "    max_context_chunks=6   # Max chunks to use for LLM context\n",
    ")\n",
    "\n",
    "# Initialize ThesisChat\n",
    "chat = ThesisChat(\n",
    "    pinecone_api_key=PINECONE_API_KEY,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    index_name=\"my-thesis-chat\",\n",
    "    namespace=\"v1\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"ThesisChat initialized: {chat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup the System\n",
    "\n",
    "This creates the Pinecone index and prepares the query engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the thesis chat system\n",
    "chat.setup(force_recreate_index=False)\n",
    "\n",
    "# Check current index stats\n",
    "stats = chat.get_index_stats()\n",
    "print(\"Index statistics:\")\n",
    "print(f\"  Total vectors: {stats['total_vector_count']}\")\n",
    "print(f\"  Dimension: {stats['dimension']}\")\n",
    "print(f\"  Metric: {stats['metric']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process and Index Your LaTeX Document\n",
    "\n",
    "This is the main processing step that converts your thesis into searchable chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the LaTeX file and create embeddings\n",
    "try:\n",
    "    chat.process_and_index_latex(LATEX_FILE_PATH)\n",
    "    print(\"‚úÖ LaTeX document processed and indexed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing document: {str(e)}\")\n",
    "    # You might want to handle specific errors here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary of processed chunks\n",
    "summary = chat.get_chunk_summary()\n",
    "print(\"\\nüìä Document Processing Summary:\")\n",
    "print(f\"  Total chunks: {summary['total_chunks']}\")\n",
    "print(f\"  Unique chapters: {summary['unique_chapters']}\")\n",
    "print(f\"  Has embeddings: {summary['has_embeddings']}\")\n",
    "print(f\"  Is indexed: {summary['is_indexed']}\")\n",
    "\n",
    "print(\"\\nüìë Chunk types:\")\n",
    "for chunk_type, count in summary['chunk_types'].items():\n",
    "    print(f\"  {chunk_type}: {count}\")\n",
    "\n",
    "print(\"\\nüìö Thesis parts:\")\n",
    "for part, count in summary['thesis_parts'].items():\n",
    "    print(f\"  {part}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query Your Document\n",
    "\n",
    "Now you can ask questions about your thesis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question about your thesis\n",
    "question = \"What are the main contributions of this thesis?\"\n",
    "\n",
    "response = chat.query(\n",
    "    question=question,\n",
    "    language=\"auto\",\n",
    "    include_sources=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(f\"ü§î Question: {response['query']}\")\n",
    "print(f\"\\nü§ñ Answer:\\n{response['answer']}\")\n",
    "print(f\"\\nüìà Retrieved {response['chunks_retrieved']} chunks, used {response['chunks_used']} for context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sources used in the answer\n",
    "print(\"\\nüìö Sources used:\")\n",
    "for source in response['sources']:\n",
    "    print(f\"  [{source['index']}] {source['reference']}\")\n",
    "    print(f\"      Similarity: {source['similarity_score']:.3f}, Rerank: {source['rerank_score']:.3f}\")\n",
    "    print(f\"      Preview: {source['text_preview']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Try More Questions\n",
    "\n",
    "Ask different types of questions to explore your thesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different questions\n",
    "questions = [\n",
    "    \"What methodology was used in this research?\",\n",
    "    \"What were the main findings?\",\n",
    "    \"What are the limitations of this study?\",\n",
    "    \"What future work is suggested?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ü§î {q}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    try:\n",
    "        response = chat.query(q, temperature=0.7)\n",
    "        print(f\"ü§ñ {response['answer'][:500]}...\" if len(response['answer']) > 500 else response['answer'])\n",
    "        print(f\"\\nüìä Used {response['chunks_used']} chunks\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Search Without LLM Response\n",
    "\n",
    "Sometimes you just want to find relevant chunks without generating a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for relevant chunks\n",
    "search_query = \"machine learning methods\"\n",
    "search_results = chat.search(search_query, top_k=5)\n",
    "\n",
    "print(f\"üîç Search results for: '{search_query}'\")\n",
    "print(f\"Found {len(search_results)} relevant chunks:\\n\")\n",
    "\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    metadata = result['metadata']\n",
    "    print(f\"[{i}] Chapter: {metadata.get('chapter', 'N/A')}\")\n",
    "    print(f\"    Section: {metadata.get('section', 'N/A')}\")\n",
    "    print(f\"    Similarity: {result.get('score', 0):.3f}\")\n",
    "    print(f\"    Rerank: {result.get('rerank_score', 0):.3f}\")\n",
    "    print(f\"    Preview: {metadata.get('text', '')[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save and Load Processed Data\n",
    "\n",
    "Save your processed chunks for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save chunks with embeddings\n",
    "output_file = \"my_thesis_chunks.jsonl\"\n",
    "chat.save_chunks(output_file, include_embeddings=True)\n",
    "print(f\"üíæ Chunks saved to: {output_file}\")\n",
    "\n",
    "# You can later load these chunks in another session:\n",
    "# chat.load_chunks(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Multilingual Support\n",
    "\n",
    "The system supports multilingual queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions in different languages\n",
    "spanish_question = \"¬øCu√°les son las principales conclusiones de esta tesis?\"\n",
    "\n",
    "response = chat.query(\n",
    "    question=spanish_question,\n",
    "    language=\"es\",  # Spanish\n",
    "    include_sources=True\n",
    ")\n",
    "\n",
    "print(f\"ü§î Pregunta: {response['query']}\")\n",
    "print(f\"\\nü§ñ Respuesta:\\n{response['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup (Optional)\n",
    "\n",
    "If you want to clear the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clear the index\n",
    "# chat.clear_index()\n",
    "# print(\"üóëÔ∏è  Index cleared!\")\n",
    "\n",
    "print(\"‚úÖ Basic usage example completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}