{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ThesisChat - Advanced Usage Example\n",
    "\n",
    "This notebook demonstrates advanced features and customization options of the ThesisChat module.\n",
    "\n",
    "## Topics Covered\n",
    "1. Custom configuration and models\n",
    "2. Manual processing pipeline\n",
    "3. Batch processing multiple documents\n",
    "4. Custom reranking and filtering\n",
    "5. Performance optimization\n",
    "6. Integration with external systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from thesis_chat import ThesisChat, Config, LaTeXProcessor, VectorStore, QueryEngine\n",
    "from thesis_chat.utils.text_utils import TextUtils\n",
    "\n",
    "# Setup API keys\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY', 'your-pinecone-api-key')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your-openai-api-key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Configuration\n",
    "\n",
    "Create custom configurations for different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for detailed analysis (smaller chunks)\n",
    "detailed_config = Config(\n",
    "    chunk_size=200,              # Smaller chunks for detailed analysis\n",
    "    overlap=40,                  # Less overlap\n",
    "    keep_captions=True,\n",
    "    embedding_model=\"sentence-transformers/all-mpnet-base-v2\",  # Alternative model\n",
    "    reranker_model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\",    # More accurate reranker\n",
    "    llm_model=\"gpt-4\",          # More powerful LLM\n",
    "    max_context_chunks=8,        # More context\n",
    "    top_k_retrieval=100         # Retrieve more candidates\n",
    ")\n",
    "\n",
    "# Configuration for quick overview (larger chunks)\n",
    "overview_config = Config(\n",
    "    chunk_size=500,              # Larger chunks for overview\n",
    "    overlap=100,\n",
    "    keep_captions=False,         # Skip captions for speed\n",
    "    llm_model=\"gpt-3.5-turbo\",  # Faster model\n",
    "    max_context_chunks=4,        # Less context\n",
    "    top_k_retrieval=30          # Fewer candidates\n",
    ")\n",
    "\n",
    "print(\"Custom configurations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manual Processing Pipeline\n",
    "\n",
    "Use individual components for fine-grained control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components separately\n",
    "latex_processor = LaTeXProcessor(\n",
    "    chunk_size=300,\n",
    "    overlap=60,\n",
    "    keep_captions=True\n",
    ")\n",
    "\n",
    "vector_store = VectorStore(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    index_name=\"advanced-thesis-chat\",\n",
    "    namespace=\"manual-processing\"\n",
    ")\n",
    "\n",
    "print(\"Individual components initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Process LaTeX file\n",
    "latex_file = \"path/to/your/thesis.tex\"\n",
    "\n",
    "try:\n",
    "    chunks = latex_processor.process_latex_file(latex_file)\n",
    "    print(f\"‚úÖ Processed {len(chunks)} chunks\")\n",
    "    \n",
    "    # Analyze chunk distribution\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.type] = chunk_types.get(chunk.type, 0) + 1\n",
    "    \n",
    "    print(\"Chunk distribution:\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    chunks = []  # Fallback for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create custom embeddings with preprocessing\n",
    "if chunks:\n",
    "    # Custom text preprocessing before embedding\n",
    "    for chunk in chunks:\n",
    "        # Additional cleaning\n",
    "        chunk.text = TextUtils.clean_text(chunk.text)\n",
    "        \n",
    "        # Add metadata-based context\n",
    "        context_parts = []\n",
    "        if chunk.chapter:\n",
    "            context_parts.append(f\"Chapter: {chunk.chapter}\")\n",
    "        if chunk.section:\n",
    "            context_parts.append(f\"Section: {chunk.section}\")\n",
    "        if chunk.thesis_part:\n",
    "            context_parts.append(f\"Part: {chunk.thesis_part}\")\n",
    "        \n",
    "        # Prepend context to text for better embeddings\n",
    "        if context_parts:\n",
    "            chunk.text = \" | \".join(context_parts) + \"\\n\\n\" + chunk.text\n",
    "    \n",
    "    print(\"‚úÖ Applied custom preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create and manage vector store\n",
    "if chunks:\n",
    "    vector_store.create_index(force_recreate=True)\n",
    "    vector_store.load_embedding_model()\n",
    "    \n",
    "    # Create embeddings in batches with progress tracking\n",
    "    chunks_with_embeddings = vector_store.create_embeddings(chunks)\n",
    "    \n",
    "    # Upsert to Pinecone\n",
    "    vector_store.upsert_chunks(chunks_with_embeddings, batch_size=100)\n",
    "    \n",
    "    print(\"‚úÖ Vector store created and populated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Query Techniques\n",
    "\n",
    "Implement custom query strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize query engine\n",
    "query_engine = QueryEngine(\n",
    "    vector_store=vector_store,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    max_context_chunks=6\n",
    ")\n",
    "\n",
    "# Custom query function with filtering\n",
    "def advanced_query(question, chapter_filter=None, thesis_part_filter=None):\n",
    "    # Step 1: Get initial results\n",
    "    results = query_engine.search_only(question, top_k_retrieval=100, top_k_rerank=20)\n",
    "    \n",
    "    # Step 2: Apply custom filters\n",
    "    filtered_results = []\n",
    "    for result in results:\n",
    "        metadata = result['metadata']\n",
    "        \n",
    "        # Chapter filter\n",
    "        if chapter_filter and metadata.get('chapter_key') != chapter_filter:\n",
    "            continue\n",
    "        \n",
    "        # Thesis part filter\n",
    "        if thesis_part_filter and metadata.get('thesis_part') != thesis_part_filter:\n",
    "            continue\n",
    "        \n",
    "        filtered_results.append(result)\n",
    "    \n",
    "    # Step 3: Generate response with filtered context\n",
    "    if filtered_results:\n",
    "        # Use top filtered results for context\n",
    "        context_chunks = filtered_results[:6]\n",
    "        \n",
    "        # Build context manually\n",
    "        contexts = []\n",
    "        for i, chunk in enumerate(context_chunks, 1):\n",
    "            metadata = chunk['metadata']\n",
    "            text = metadata.get('text', '')\n",
    "            chapter = metadata.get('chapter', 'Unknown')\n",
    "            contexts.append(f\"[{i}] Chapter: {chapter}\\n{text[:500]}...\")\n",
    "        \n",
    "        return {\n",
    "            'filtered_chunks': len(filtered_results),\n",
    "            'context_used': len(context_chunks),\n",
    "            'contexts': contexts[:3]  # Show first 3 for demo\n",
    "        }\n",
    "    else:\n",
    "        return {'error': 'No results after filtering'}\n",
    "\n",
    "# Test advanced query\n",
    "result = advanced_query(\n",
    "    \"What are the main findings?\",\n",
    "    thesis_part_filter=\"Methods/Results\"\n",
    ")\n",
    "\n",
    "print(f\"Advanced query result: {result.get('filtered_chunks', 0)} chunks after filtering\")\n",
    "if 'contexts' in result:\n",
    "    print(\"\\nSample contexts:\")\n",
    "    for ctx in result['contexts']:\n",
    "        print(f\"  {ctx[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Processing Multiple Documents\n",
    "\n",
    "Process multiple theses or papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_documents(document_paths, base_index_name=\"multi-doc-chat\"):\n",
    "    \"\"\"\n",
    "    Process multiple LaTeX documents into separate namespaces.\n",
    "    \"\"\"\n",
    "    processed_docs = []\n",
    "    \n",
    "    for i, doc_path in enumerate(document_paths):\n",
    "        doc_name = Path(doc_path).stem\n",
    "        namespace = f\"doc_{i}_{doc_name}\"\n",
    "        \n",
    "        print(f\"\\nProcessing document {i+1}/{len(document_paths)}: {doc_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Create separate ThesisChat instance for each document\n",
    "            chat = ThesisChat(\n",
    "                pinecone_api_key=PINECONE_API_KEY,\n",
    "                openai_api_key=OPENAI_API_KEY,\n",
    "                index_name=base_index_name,\n",
    "                namespace=namespace\n",
    "            )\n",
    "            \n",
    "            chat.setup()\n",
    "            chat.process_and_index_latex(doc_path)\n",
    "            \n",
    "            summary = chat.get_chunk_summary()\n",
    "            processed_docs.append({\n",
    "                'name': doc_name,\n",
    "                'namespace': namespace,\n",
    "                'chunks': summary['total_chunks'],\n",
    "                'chat_instance': chat\n",
    "            })\n",
    "            \n",
    "            print(f\"  ‚úÖ Processed {summary['total_chunks']} chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error processing {doc_name}: {str(e)}\")\n",
    "    \n",
    "    return processed_docs\n",
    "\n",
    "# Example usage (uncomment when you have multiple documents)\n",
    "# document_paths = [\n",
    "#     \"path/to/thesis1.tex\",\n",
    "#     \"path/to/thesis2.tex\",\n",
    "#     \"path/to/paper1.tex\"\n",
    "# ]\n",
    "# \n",
    "# processed_docs = process_multiple_documents(document_paths)\n",
    "# print(f\"\\nüìä Processed {len(processed_docs)} documents successfully\")\n",
    "\n",
    "print(\"Batch processing function defined (uncomment to use with real documents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Optimization\n",
    "\n",
    "Techniques for optimizing performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Optimize embedding creation with parallel processing\n",
    "def create_embeddings_parallel(chunks, model, batch_size=32, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create embeddings using parallel processing for large datasets.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Split chunks into batches\n",
    "    batches = [chunks[i:i + batch_size] for i in range(0, len(chunks), batch_size)]\n",
    "    \n",
    "    def process_batch(batch):\n",
    "        texts = [chunk.text for chunk in batch]\n",
    "        embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "        \n",
    "        # Attach embeddings to chunks\n",
    "        for chunk, embedding in zip(batch, embeddings):\n",
    "            chunk.embedding = embedding.tolist()\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        processed_batches = list(executor.map(process_batch, batches))\n",
    "    \n",
    "    # Flatten results\n",
    "    all_chunks = []\n",
    "    for batch in processed_batches:\n",
    "        all_chunks.extend(batch)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"‚ö° Parallel embedding creation took {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Caching for repeated queries\n",
    "class QueryCache:\n",
    "    def __init__(self, max_size=100):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "        self.access_order = []\n",
    "    \n",
    "    def get(self, query):\n",
    "        if query in self.cache:\n",
    "            # Move to end (most recently used)\n",
    "            self.access_order.remove(query)\n",
    "            self.access_order.append(query)\n",
    "            return self.cache[query]\n",
    "        return None\n",
    "    \n",
    "    def set(self, query, result):\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            # Remove least recently used\n",
    "            oldest = self.access_order.pop(0)\n",
    "            del self.cache[oldest]\n",
    "        \n",
    "        self.cache[query] = result\n",
    "        self.access_order.append(query)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.cache.clear()\n",
    "        self.access_order.clear()\n",
    "\n",
    "# Create global cache\n",
    "query_cache = QueryCache(max_size=50)\n",
    "\n",
    "def cached_query(chat_instance, question, **kwargs):\n",
    "    \"\"\"\n",
    "    Query with caching for repeated questions.\n",
    "    \"\"\"\n",
    "    cache_key = f\"{question}_{str(sorted(kwargs.items()))}\"\n",
    "    \n",
    "    # Check cache first\n",
    "    cached_result = query_cache.get(cache_key)\n",
    "    if cached_result:\n",
    "        print(\"üöÄ Returning cached result\")\n",
    "        return cached_result\n",
    "    \n",
    "    # Perform actual query\n",
    "    result = chat_instance.query(question, **kwargs)\n",
    "    \n",
    "    # Cache the result\n",
    "    query_cache.set(cache_key, result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚ö° Performance optimization tools defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Analysis and Reporting\n",
    "\n",
    "Generate detailed analysis reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_thesis_analysis_report(chat_instance, output_file=\"thesis_analysis.json\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive analysis report of the thesis.\n",
    "    \"\"\"\n",
    "    print(\"üîç Generating comprehensive thesis analysis...\")\n",
    "    \n",
    "    # Predefined analysis questions\n",
    "    analysis_questions = {\n",
    "        \"research_objectives\": \"What are the main research objectives and goals?\",\n",
    "        \"methodology\": \"What methodology and approach were used in this research?\",\n",
    "        \"key_findings\": \"What are the key findings and results?\",\n",
    "        \"contributions\": \"What are the main contributions of this work?\",\n",
    "        \"limitations\": \"What are the limitations of this study?\",\n",
    "        \"future_work\": \"What future work or research directions are suggested?\",\n",
    "        \"related_work\": \"What related work and previous research is discussed?\",\n",
    "        \"conclusions\": \"What are the main conclusions?\"\n",
    "    }\n",
    "    \n",
    "    analysis_results = {}\n",
    "    \n",
    "    for category, question in analysis_questions.items():\n",
    "        print(f\"  Analyzing: {category}\")\n",
    "        \n",
    "        try:\n",
    "            response = chat_instance.query(\n",
    "                question,\n",
    "                temperature=0.3,  # Lower temperature for more consistent analysis\n",
    "                include_sources=True\n",
    "            )\n",
    "            \n",
    "            analysis_results[category] = {\n",
    "                \"question\": question,\n",
    "                \"answer\": response[\"answer\"],\n",
    "                \"chunks_used\": response[\"chunks_used\"],\n",
    "                \"sources\": [{\n",
    "                    \"reference\": src[\"reference\"],\n",
    "                    \"chapter\": src[\"chapter\"],\n",
    "                    \"similarity_score\": src[\"similarity_score\"]\n",
    "                } for src in response[\"sources\"][:3]]  # Top 3 sources\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            analysis_results[category] = {\n",
    "                \"question\": question,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    # Add document statistics\n",
    "    summary = chat_instance.get_chunk_summary()\n",
    "    analysis_results[\"document_stats\"] = summary\n",
    "    \n",
    "    # Save report\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Analysis report saved to: {output_file}\")\n",
    "    return analysis_results\n",
    "\n",
    "# Function to create a summary visualization\n",
    "def create_analysis_summary(analysis_results):\n",
    "    \"\"\"\n",
    "    Create a text-based summary of the analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìã THESIS ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Document statistics\n",
    "    if \"document_stats\" in analysis_results:\n",
    "        stats = analysis_results[\"document_stats\"]\n",
    "        print(f\"üìä Document Statistics:\")\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\")\n",
    "        print(f\"  Unique chapters: {stats['unique_chapters']}\")\n",
    "        print(f\"  Thesis parts: {list(stats['thesis_parts'].keys())}\")\n",
    "        print()\n",
    "    \n",
    "    # Key insights\n",
    "    key_categories = [\"research_objectives\", \"key_findings\", \"contributions\", \"conclusions\"]\n",
    "    \n",
    "    for category in key_categories:\n",
    "        if category in analysis_results and \"answer\" in analysis_results[category]:\n",
    "            print(f\"üîç {category.replace('_', ' ').title()}:\")\n",
    "            answer = analysis_results[category][\"answer\"]\n",
    "            # Truncate long answers\n",
    "            if len(answer) > 300:\n",
    "                answer = answer[:300] + \"...\"\n",
    "            print(f\"  {answer}\")\n",
    "            print()\n",
    "\n",
    "print(\"üìä Analysis and reporting tools defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration with External Tools\n",
    "\n",
    "Examples of integrating with other systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data for external analysis\n",
    "def export_for_analysis(chat_instance, format=\"csv\"):\n",
    "    \"\"\"\n",
    "    Export chunk data for external analysis tools.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Get chunk data\n",
    "    chunks = chat_instance.chunks\n",
    "    \n",
    "    # Convert to structured data\n",
    "    data = []\n",
    "    for chunk in chunks:\n",
    "        data.append({\n",
    "            'id': chunk.id,\n",
    "            'text': chunk.text,\n",
    "            'type': chunk.type,\n",
    "            'chapter_key': chunk.chapter_key,\n",
    "            'chapter': chunk.chapter,\n",
    "            'section': chunk.section,\n",
    "            'thesis_part': chunk.thesis_part,\n",
    "            'word_count': len(chunk.text.split()),\n",
    "            'chunk_idx': chunk.chunk_idx,\n",
    "            'chunk_total': chunk.chunk_total\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    if format == \"csv\":\n",
    "        output_file = \"thesis_chunks_export.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "    elif format == \"excel\":\n",
    "        output_file = \"thesis_chunks_export.xlsx\"\n",
    "        df.to_excel(output_file, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Format must be 'csv' or 'excel'\")\n",
    "    \n",
    "    print(f\"üì§ Data exported to: {output_file}\")\n",
    "    return df\n",
    "\n",
    "# Create API endpoint wrapper\n",
    "def create_api_wrapper(chat_instance):\n",
    "    \"\"\"\n",
    "    Create a simple API wrapper for the chat instance.\n",
    "    \"\"\"\n",
    "    from flask import Flask, request, jsonify\n",
    "    \n",
    "    app = Flask(__name__)\n",
    "    \n",
    "    @app.route('/query', methods=['POST'])\n",
    "    def api_query():\n",
    "        data = request.json\n",
    "        question = data.get('question', '')\n",
    "        language = data.get('language', 'auto')\n",
    "        temperature = data.get('temperature', 0.7)\n",
    "        \n",
    "        try:\n",
    "            response = chat_instance.query(\n",
    "                question=question,\n",
    "                language=language,\n",
    "                temperature=temperature,\n",
    "                include_sources=True\n",
    "            )\n",
    "            return jsonify(response)\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    @app.route('/search', methods=['POST'])\n",
    "    def api_search():\n",
    "        data = request.json\n",
    "        query = data.get('query', '')\n",
    "        top_k = data.get('top_k', 10)\n",
    "        \n",
    "        try:\n",
    "            results = chat_instance.search(query, top_k=top_k)\n",
    "            return jsonify({'results': results})\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    @app.route('/stats', methods=['GET'])\n",
    "    def api_stats():\n",
    "        try:\n",
    "            summary = chat_instance.get_chunk_summary()\n",
    "            index_stats = chat_instance.get_index_stats()\n",
    "            return jsonify({\n",
    "                'chunk_summary': summary,\n",
    "                'index_stats': index_stats\n",
    "            })\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Example webhook for real-time notifications\n",
    "def setup_webhook(chat_instance, webhook_url):\n",
    "    \"\"\"\n",
    "    Setup webhook for query notifications.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    \n",
    "    def notify_webhook(query_data):\n",
    "        try:\n",
    "            requests.post(webhook_url, json=query_data, timeout=5)\n",
    "        except Exception as e:\n",
    "            print(f\"Webhook notification failed: {e}\")\n",
    "    \n",
    "    # Wrapper function that sends notifications\n",
    "    original_query = chat_instance.query\n",
    "    \n",
    "    def query_with_notification(*args, **kwargs):\n",
    "        result = original_query(*args, **kwargs)\n",
    "        \n",
    "        # Send notification\n",
    "        notification_data = {\n",
    "            'timestamp': time.time(),\n",
    "            'query': result.get('query', ''),\n",
    "            'chunks_used': result.get('chunks_used', 0),\n",
    "            'success': True\n",
    "        }\n",
    "        notify_webhook(notification_data)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Replace the query method\n",
    "    chat_instance.query = query_with_notification\n",
    "    \n",
    "    print(f\"üîó Webhook notifications enabled for: {webhook_url}\")\n",
    "\n",
    "print(\"üîå Integration tools defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Putting It All Together\n",
    "\n",
    "Example of using advanced features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete advanced workflow\n",
    "def run_advanced_workflow(latex_file):\n",
    "    \"\"\"\n",
    "    Demonstrate a complete advanced workflow.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting advanced workflow...\")\n",
    "    \n",
    "    # Step 1: Initialize with detailed configuration\n",
    "    chat = ThesisChat(\n",
    "        pinecone_api_key=PINECONE_API_KEY,\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "        index_name=\"advanced-workflow\",\n",
    "        namespace=\"detailed-analysis\",\n",
    "        config=detailed_config\n",
    "    )\n",
    "    \n",
    "    # Step 2: Setup and process\n",
    "    chat.setup(force_recreate_index=True)\n",
    "    \n",
    "    try:\n",
    "        chat.process_and_index_latex(latex_file)\n",
    "        print(\"‚úÖ Document processed successfully\")\n",
    "        \n",
    "        # Step 3: Generate comprehensive analysis\n",
    "        analysis = generate_thesis_analysis_report(chat)\n",
    "        create_analysis_summary(analysis)\n",
    "        \n",
    "        # Step 4: Export data for external analysis\n",
    "        export_df = export_for_analysis(chat, format=\"csv\")\n",
    "        print(f\"üìä Exported {len(export_df)} chunks for analysis\")\n",
    "        \n",
    "        # Step 5: Test performance optimization\n",
    "        test_questions = [\n",
    "            \"What is the main research question?\",\n",
    "            \"What methodology was used?\",\n",
    "            \"What are the key findings?\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nüîç Testing cached queries:\")\n",
    "        for question in test_questions:\n",
    "            start_time = time.time()\n",
    "            result = cached_query(chat, question)\n",
    "            end_time = time.time()\n",
    "            print(f\"  Query time: {end_time - start_time:.2f}s\")\n",
    "            \n",
    "            # Query again to test cache\n",
    "            start_time = time.time()\n",
    "            result = cached_query(chat, question)\n",
    "            end_time = time.time()\n",
    "            print(f\"  Cached time: {end_time - start_time:.2f}s\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Advanced workflow completed successfully!\")\n",
    "        return chat\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Workflow failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Uncomment to run with your thesis file\n",
    "# chat_instance = run_advanced_workflow(\"path/to/your/thesis.tex\")\n",
    "\n",
    "print(\"üéØ Advanced workflow function ready!\")\n",
    "print(\"\\nüìñ This notebook demonstrated:\")\n",
    "print(\"  ‚úì Custom configurations for different use cases\")\n",
    "print(\"  ‚úì Manual processing pipeline with fine-grained control\")\n",
    "print(\"  ‚úì Advanced query techniques with filtering\")\n",
    "print(\"  ‚úì Batch processing for multiple documents\")\n",
    "print(\"  ‚úì Performance optimization with caching and parallel processing\")\n",
    "print(\"  ‚úì Comprehensive analysis and reporting\")\n",
    "print(\"  ‚úì Integration patterns for external systems\")\n",
    "print(\"\\nüöÄ You're ready to build advanced thesis chat applications!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",
n",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}