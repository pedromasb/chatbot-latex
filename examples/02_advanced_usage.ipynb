{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ThesisChat - Advanced Usage Example\n",
    "\n",
    "This notebook demonstrates advanced features and customization options of the ThesisChat module.\n",
    "\n",
    "## Topics Covered\n",
    "1. Custom configuration and models\n",
    "2. Manual processing pipeline\n",
    "3. Batch processing multiple documents\n",
    "4. Custom reranking and filtering\n",
    "5. Performance optimization\n",
    "6. Integration with external systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from thesis_chat import ThesisChat, Config, LaTeXProcessor, VectorStore, QueryEngine\n",
    "from thesis_chat.utils.text_utils import TextUtils\n",
    "\n",
    "# Setup API keys\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY', 'your-pinecone-api-key')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your-openai-api-key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Configuration\n",
    "\n",
    "Create custom configurations for different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for detailed analysis (smaller chunks)\n",
    "detailed_config = Config(\n",
    "    chunk_size=200,              # Smaller chunks for detailed analysis\n",
    "    overlap=40,                  # Less overlap\n",
    "    keep_captions=True,\n",
    "    embedding_model=\"sentence-transformers/all-mpnet-base-v2\",  # Alternative model\n",
    "    reranker_model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\",    # More accurate reranker\n",
    "    llm_model=\"gpt-4\",          # More powerful LLM\n",
    "    max_context_chunks=8,        # More context\n",
    "    top_k_retrieval=100         # Retrieve more candidates\n",
    ")\n",
    "\n",
    "# Configuration for quick overview (larger chunks)\n",
    "overview_config = Config(\n",
    "    chunk_size=500,              # Larger chunks for overview\n",
    "    overlap=100,\n",
    "    keep_captions=False,         # Skip captions for speed\n",
    "    llm_model=\"gpt-3.5-turbo\",  # Faster model\n",
    "    max_context_chunks=4,        # Less context\n",
    "    top_k_retrieval=30          # Fewer candidates\n",
    ")\n",
    "\n",
    "print(\"Custom configurations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manual Processing Pipeline\n",
    "\n",
    "Use individual components for fine-grained control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components separately\n",
    "latex_processor = LaTeXProcessor(\n",
    "    chunk_size=300,\n",
    "    overlap=60,\n",
    "    keep_captions=True\n",
    ")\n",
    "\n",
    "vector_store = VectorStore(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    index_name=\"advanced-thesis-chat\",\n",
    "    namespace=\"manual-processing\"\n",
    ")\n",
    "\n",
    "print(\"Individual components initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Process LaTeX file\n",
    "latex_file = \"path/to/your/thesis.tex\"\n",
    "\n",
    "try:\n",
    "    chunks = latex_processor.process_latex_file(latex_file)\n",
    "    print(f\"✅ Processed {len(chunks)} chunks\")\n",
    "    \n",
    "    # Analyze chunk distribution\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.type] = chunk_types.get(chunk.type, 0) + 1\n",
    "    \n",
    "    print(\"Chunk distribution:\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    chunks = []  # Fallback for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create custom embeddings with preprocessing\n",
    "if chunks:\n",
    "    # Custom text preprocessing before embedding\n",
    "    for chunk in chunks:\n",
    "        # Additional cleaning\n",
    "        chunk.text = TextUtils.clean_text(chunk.text)\n",
    "        \n",
    "        # Add metadata-based context\n",
    "        context_parts = []\n",
    "        if chunk.chapter:\n",
    "            context_parts.append(f\"Chapter: {chunk.chapter}\")\n",
    "        if chunk.section:\n",
    "            context_parts.append(f\"Section: {chunk.section}\")\n",
    "        if chunk.thesis_part:\n",
    "            context_parts.append(f\"Part: {chunk.thesis_part}\")\n",
    "        \n",
    "        # Prepend context to text for better embeddings\n",
    "        if context_parts:\n",
    "            chunk.text = \" | \".join(context_parts) + \"\\n\\n\" + chunk.text\n",
    "    \n",
    "    print(\"✅ Applied custom preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create and manage vector store\n",
    "if chunks:\n",
    "    vector_store.create_index(force_recreate=True)\n",
    "    vector_store.load_embedding_model()\n",
    "    \n",
    "    # Create embeddings in batches with progress tracking\n",
    "    chunks_with_embeddings = vector_store.create_embeddings(chunks)\n",
    "    \n",
    "    # Upsert to Pinecone\n",
    "    vector_store.upsert_chunks(chunks_with_embeddings, batch_size=100)\n",
    "    \n",
    "    print(\"✅ Vector store created and populated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Query Techniques\n",
    "\n",
    "Implement custom query strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize query engine\n",
    "query_engine = QueryEngine(\n",
    "    vector_store=vector_store,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    max_context_chunks=6\n",
    ")\n",
    "\n",
    "# Custom query function with filtering\n",
    "def advanced_query(question, chapter_filter=None, thesis_part_filter=None):\n",
    "    # Step 1: Get initial results\n",
    "    results = query_engine.search_only(question, top_k_retrieval=100, top_k_rerank=20)\n",
    "    \n",
    "    # Step 2: Apply custom filters\n",
    "    filtered_results = []\n",
    "    for result in results:\n",
    "        metadata = result['metadata']\n",
    "        \n",
    "        # Chapter filter\n",
    "        if chapter_filter and metadata.get('chapter_key') != chapter_filter:\n",
    "            continue\n",
    "        \n",
    "        # Thesis part filter\n",
    "        if thesis_part_filter and metadata.get('thesis_part') != thesis_part_filter:\n",
    "            continue\n",
    "        \n",
    "        filtered_results.append(result)\n",
    "    \n",
    "    # Step 3: Generate response with filtered context\n",
    "    if filtered_results:\n",
    "        # Use top filtered results for context\n",
    "        context_chunks = filtered_results[:6]\n",
    "        \n",
    "        # Build context manually\n",
    "        contexts = []\n",
    "        for i, chunk in enumerate(context_chunks, 1):\n",
    "            metadata = chunk['metadata']\n",
    "            text = metadata.get('text', '')\n",
    "            chapter = metadata.get('chapter', 'Unknown')\n",
    "            contexts.append(f\"[{i}] Chapter: {chapter}\\n{text[:500]}...\")\n",
    "        \n",
    "        return {\n",
    "            'filtered_chunks': len(filtered_results),\n",
    "            'context_used': len(context_chunks),\n",
    "            'contexts': contexts[:3]  # Show first 3 for demo\n",
    "        }\n",
    "    else:\n",
    "        return {'error': 'No results after filtering'}\n",
    "\n",
    "# Test advanced query\n",
    "result = advanced_query(\n",
    "    \"What are the main findings?\",\n",
    "    thesis_part_filter=\"Methods/Results\"\n",
    ")\n",
    "\n",
    "print(f\"Advanced query result: {result.get('filtered_chunks', 0)} chunks after filtering\")\n",
    "if 'contexts' in result:\n",
    "    print(\"\\nSample contexts:\")\n",
    "    for ctx in result['contexts']:\n",
    "        print(f\"  {ctx[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Processing Multiple Documents\n",
    "\n",
    "Process multiple theses or papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_documents(document_paths, base_index_name=\"multi-doc-chat\"):\n",
    "    \"\"\"\n",
    "    Process multiple LaTeX documents into separate namespaces.\n",
    "    \"\"\"\n",
    "    processed_docs = []\n",
    "    \n",
    "    for i, doc_path in enumerate(document_paths):\n",
    "        doc_name = Path(doc_path).stem\n",
    "        namespace = f\"doc_{i}_{doc_name}\"\n",
    "        \n",
    "        print(f\"\\nProcessing document {i+1}/{len(document_paths)}: {doc_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Create separate ThesisChat instance for each document\n",
    "            chat = ThesisChat(\n",
    "                pinecone_api_key=PINECONE_API_KEY,\n",
    "                openai_api_key=OPENAI_API_KEY,\n",
    "                index_name=base_index_name,\n",
    "                namespace=namespace\n",
    "            )\n",
    "            \n",
    "            chat.setup()\n",
    "            chat.process_and_index_latex(doc_path)\n",
    "            \n",
    "            summary = chat.get_chunk_summary()\n",
    "            processed_docs.append({\n",
    "                'name': doc_name,\n",
    "                'namespace': namespace,\n",
    "                'chunks': summary['total_chunks'],\n",
    "                'chat_instance': chat\n",
    "            })\n",
    "            \n",
    "            print(f\"  ✅ Processed {summary['total_chunks']} chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error processing {doc_name}: {str(e)}\")\n",
    "    \n",
    "    return processed_docs\n",
    "\n",
    "# Example usage (uncomment when you have multiple documents)\n",
    "# document_paths = [\n",
    "#     \"path/to/thesis1.tex\",\n",
    "#     \"path/to/thesis2.tex\",\n",
    "#     \"path/to/paper1.tex\"\n",
    "# ]\n",
    "# \n",
    "# processed_docs = process_multiple_documents(document_paths)\n",
    "# print(f\"\\n📊 Processed {len(processed_docs)} documents successfully\")\n",
    "\n",
    "print(\"Batch processing function defined (uncomment to use with real documents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Optimization\n",
    "\n",
    "Techniques for optimizing performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Optimize embedding creation with parallel processing\n",
    "def create_embeddings_parallel(chunks, model, batch_size=32, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create embeddings using parallel processing for large datasets.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Split chunks into batches\n",
    "    batches = [chunks[i:i + batch_size] for i in range(0, len(chunks), batch_size)]\n",
    "    \n",
    "    def process_batch(batch):\n",
    "        texts = [chunk.text for chunk in batch]\n",
    "        embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "        \n",
    "        # Attach embeddings to chunks\n",
    "        for chunk, embedding in zip(batch, embeddings):\n",
    "            chunk.embedding = embedding.tolist()\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        processed_batches = list(executor.map(process_batch, batches))\n",
    "    \n",
    "    # Flatten results\n",
    "    all_chunks = []\n",
    "    for batch in processed_batches:\n",
    "        all_chunks.extend(batch)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"⚡ Parallel embedding creation took {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Caching for repeated queries\n",
    "class QueryCache:\n",
    "    def __init__(self, max_size=100):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "        self.access_order = []\n",
    "    \n",
    "    def get(self, query):\n",
    "        if query in self.cache:\n",
    "            # Move to end (most recently used)\n",
    "            self.access_order.remove(query)\n",
    "            self.access_order.append(query)\n",
    "            return self.cache[query]\n",
    "        return None\n",
    "    \n",
    "    def set(self, query, result):\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            # Remove least recently used\n",
    "            oldest = self.access_order.pop(0)\n",
    "            del self.cache[oldest]\n",
    "        \n",
    "        self.cache[query] = result\n",
    "        self.access_order.append(query)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.cache.clear()\n",
    "        self.access_order.clear()\n",
    "\n",
    "# Create global cache\n",
    "query_cache = QueryCache(max_size=50)\n",
    "\n",
    "def cached_query(chat_instance, question, **kwargs):\n",
    "    \"\"\"\n",
    "    Query with caching for repeated questions.\n",
    "    \"\"\"\n",
    "    cache_key = f\"{question}_{str(sorted(kwargs.items()))}\"\n",
    "    \n",
    "    # Check cache first\n",
    "    cached_result = query_cache.get(cache_key)\n",
    "    if cached_result:\n",
    "        print(\"🚀 Returning cached result\")\n",
    "        return cached_result\n",
    "    \n",
    "    # Perform actual query\n",
    "    result = chat_instance.query(question, **kwargs)\n",
    "    \n",
    "    # Cache the result\n",
    "    query_cache.set(cache_key, result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"⚡ Performance optimization tools defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Analysis and Reporting\n",
    "\n",
    "Generate detailed analysis reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_thesis_analysis_report(chat_instance, output_file=\"thesis_analysis.json\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive analysis report of the thesis.\n",
    "    \"\"\"\n",
    "    print(\"🔍 Generating comprehensive thesis analysis...\")\n",
    "    \n",
    "    # Predefined analysis questions\n",
    "    analysis_questions = {\n",
    "        \"research_objectives\": \"What are the main research objectives and goals?\",\n",
    "        \"methodology\": \"What methodology and approach were used in this research?\",\n",
    "        \"key_findings\": \"What are the key findings and results?\",\n",
    "        \"contributions\": \"What are the main contributions of this work?\",\n",
    "        \"limitations\": \"What are the limitations of this study?\",\n",
    "        \"future_work\": \"What future work or research directions are suggested?\",\n",
    "        \"related_work\": \"What related work and previous research is discussed?\",\n",
    "        \"conclusions\": \"What are the main conclusions?\"\n",
    "    }\n",
    "    \n",
    "    analysis_results = {}\n",
    "    \n",
    "    for category, question in analysis_questions.items():\n",
    "        print(f\"  Analyzing: {category}\")\n",
    "        \n",
    "        try:\n",
    "            response = chat_instance.query(\n",
    "                question,\n",
    "                temperature=0.3,  # Lower temperature for more consistent analysis\n",
    "                include_sources=True\n",
    "            )\n",
    "            \n",
    "            analysis_results[category] = {\n",
    "                \"question\": question,\n",
    "                \"answer\": response[\"answer\"],\n",
    "                \"chunks_used\": response[\"chunks_used\"],\n",
    "                \"sources\": [{\n",
    "                    \"reference\": src[\"reference\"],\n",
    "                    \"chapter\": src[\"chapter\"],\n",
    "                    \"similarity_score\": src[\"similarity_score\"]\n",
    "                } for src in response[\"sources\"][:3]]  # Top 3 sources\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            analysis_results[category] = {\n",
    "                \"question\": question,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    # Add document statistics\n",
    "    summary = chat_instance.get_chunk_summary()\n",
    "    analysis_results[\"document_stats\"] = summary\n",
    "    \n",
    "    # Save report\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ Analysis report saved to: {output_file}\")\n",
    "    return analysis_results\n",
    "\n",
    "# Function to create a summary visualization\n",
    "def create_analysis_summary(analysis_results):\n",
    "    \"\"\"\n",
    "    Create a text-based summary of the analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n📋 THESIS ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Document statistics\n",
    "    if \"document_stats\" in analysis_results:\n",
    "        stats = analysis_results[\"document_stats\"]\n",
    "        print(f\"📊 Document Statistics:\")\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\")\n",
    "        print(f\"  Unique chapters: {stats['unique_chapters']}\")\n",
    "        print(f\"  Thesis parts: {list(stats['thesis_parts'].keys())}\")\n",
    "        print()\n",
    "    \n",
    "    # Key insights\n",
    "    key_categories = [\"research_objectives\", \"key_findings\", \"contributions\", \"conclusions\"]\n",
    "    \n",
    "    for category in key_categories:\n",
    "        if category in analysis_results and \"answer\" in analysis_results[category]:\n",
    "            print(f\"🔍 {category.replace('_', ' ').title()}:\")\n",
    "            answer = analysis_results[category][\"answer\"]\n",
    "            # Truncate long answers\n",
    "            if len(answer) > 300:\n",
    "                answer = answer[:300] + \"...\"\n",
    "            print(f\"  {answer}\")\n",
    "            print()\n",
    "\n",
    "print(\"📊 Analysis and reporting tools defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration with External Tools\n",
    "\n",
    "Examples of integrating with other systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data for external analysis\n",
    "def export_for_analysis(chat_instance, format=\"csv\"):\n",
    "    \"\"\"\n",
    "    Export chunk data for external analysis tools.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Get chunk data\n",
    "    chunks = chat_instance.chunks\n",
    "    \n",
    "    # Convert to structured data\n",
    "    data = []\n",
    "    for chunk in chunks:\n",
    "        data.append({\n",
    "            'id': chunk.id,\n",
    "            'text': chunk.text,\n",
    "            'type': chunk.type,\n",
    "            'chapter_key': chunk.chapter_key,\n",
    "            'chapter': chunk.chapter,\n",
    "            'section': chunk.section,\n",
    "            'thesis_part': chunk.thesis_part,\n",
    "            'word_count': len(chunk.text.split()),\n",
    "            'chunk_idx': chunk.chunk_idx,\n",
    "            'chunk_total': chunk.chunk_total\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    if format == \"csv\":\n",
    "        output_file = \"thesis_chunks_export.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "    elif format == \"excel\":\n",
    "        output_file = \"thesis_chunks_export.xlsx\"\n",
    "        df.to_excel(output_file, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Format must be 'csv' or 'excel'\")\n",
    "    \n",
    "    print(f\"📤 Data exported to: {output_file}\")\n",
    "    return df\n",
    "\n",
    "# Create API endpoint wrapper\n",
    "def create_api_wrapper(chat_instance):\n",
    "    \"\"\"\n",
    "    Create a simple API wrapper for the chat instance.\n",
    "    \"\"\"\n",
    "    from flask import Flask, request, jsonify\n",
    "    \n",
    "    app = Flask(__name__)\n",
    "    \n",
    "    @app.route('/query', methods=['POST'])\n",
    "    def api_query():\n",
    "        data = request.json\n",
    "        question = data.get('question', '')\n",
    "        language = data.get('language', 'auto')\n",
    "        temperature = data.get('temperature', 0.7)\n",
    "        \n",
    "        try:\n",
    "            response = chat_instance.query(\n",
    "                question=question,\n",
    "                language=language,\n",
    "                temperature=temperature,\n",
    "                include_sources=True\n",
    "            )\n",
    "            return jsonify(response)\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    @app.route('/search', methods=['POST'])\n",
    "    def api_search():\n",
    "        data = request.json\n",
    "        query = data.get('query', '')\n",
    "        top_k = data.get('top_k', 10)\n",
    "        \n",
    "        try:\n",
    "            results = chat_instance.search(query, top_k=top_k)\n",
    "            return jsonify({'results': results})\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    @app.route('/stats', methods=['GET'])\n",
    "    def api_stats():\n",
    "        try:\n",
    "            summary = chat_instance.get_chunk_summary()\n",
    "            index_stats = chat_instance.get_index_stats()\n",
    "            return jsonify({\n",
    "                'chunk_summary': summary,\n",
    "                'index_stats': index_stats\n",
    "            })\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Example webhook for real-time notifications\n",
    "def setup_webhook(chat_instance, webhook_url):\n",
    "    \"\"\"\n",
    "    Setup webhook for query notifications.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    \n",
    "    def notify_webhook(query_data):\n",
    "        try:\n",
    "            requests.post(webhook_url, json=query_data, timeout=5)\n",
    "        except Exception as e:\n",
    "            print(f\"Webhook notification failed: {e}\")\n",
    "    \n",
    "    # Wrapper function that sends notifications\n",
    "    original_query = chat_instance.query\n",
    "    \n",
    "    def query_with_notification(*args, **kwargs):\n",
    "        result = original_query(*args, **kwargs)\n",
    "        \n",
    "        # Send notification\n",
    "        notification_data = {\n",
    "            'timestamp': time.time(),\n",
    "            'query': result.get('query', ''),\n",
    "            'chunks_used': result.get('chunks_used', 0),\n",
    "            'success': True\n",
    "        }\n",
    "        notify_webhook(notification_data)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Replace the query method\n",
    "    chat_instance.query = query_with_notification\n",
    "    \n",
    "    print(f\"🔗 Webhook notifications enabled for: {webhook_url}\")\n",
    "\n",
    "print(\"🔌 Integration tools defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Putting It All Together\n",
    "\n",
    "Example of using advanced features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete advanced workflow\n",
    "def run_advanced_workflow(latex_file):\n",
    "    \"\"\"\n",
    "    Demonstrate a complete advanced workflow.\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting advanced workflow...\")\n",
    "    \n",
    "    # Step 1: Initialize with detailed configuration\n",
    "    chat = ThesisChat(\n",
    "        pinecone_api_key=PINECONE_API_KEY,\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "        index_name=\"advanced-workflow\",\n",
    "        namespace=\"detailed-analysis\",\n",
    "        config=detailed_config\n",
    "    )\n",
    "    \n",
    "    # Step 2: Setup and process\n",
    "    chat.setup(force_recreate_index=True)\n",
    "    \n",
    "    try:\n",
    "        chat.process_and_index_latex(latex_file)\n",
    "        print(\"✅ Document processed successfully\")\n",
    "        \n",
    "        # Step 3: Generate comprehensive analysis\n",
    "        analysis = generate_thesis_analysis_report(chat)\n",
    "        create_analysis_summary(analysis)\n",
    "        \n",
    "        # Step 4: Export data for external analysis\n",
    "        export_df = export_for_analysis(chat, format=\"csv\")\n",
    "        print(f\"📊 Exported {len(export_df)} chunks for analysis\")\n",
    "        \n",
    "        # Step 5: Test performance optimization\n",
    "        test_questions = [\n",
    "            \"What is the main research question?\",\n",
    "            \"What methodology was used?\",\n",
    "            \"What are the key findings?\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n🔍 Testing cached queries:\")\n",
    "        for question in test_questions:\n",
    "            start_time = time.time()\n",
    "            result = cached_query(chat, question)\n",
    "            end_time = time.time()\n",
    "            print(f\"  Query time: {end_time - start_time:.2f}s\")\n",
    "            \n",
    "            # Query again to test cache\n",
    "            start_time = time.time()\n",
    "            result = cached_query(chat, question)\n",
    "            end_time = time.time()\n",
    "            print(f\"  Cached time: {end_time - start_time:.2f}s\")\n",
    "        \n",
    "        print(\"\\n✅ Advanced workflow completed successfully!\")\n",
    "        return chat\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Workflow failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Uncomment to run with your thesis file\n",
    "# chat_instance = run_advanced_workflow(\"path/to/your/thesis.tex\")\n",
    "\n",
    "print(\"🎯 Advanced workflow function ready!\")\n",
    "print(\"\\n📖 This notebook demonstrated:\")\n",
    "print(\"  ✓ Custom configurations for different use cases\")\n",
    "print(\"  ✓ Manual processing pipeline with fine-grained control\")\n",
    "print(\"  ✓ Advanced query techniques with filtering\")\n",
    "print(\"  ✓ Batch processing for multiple documents\")\n",
    "print(\"  ✓ Performance optimization with caching and parallel processing\")\n",
    "print(\"  ✓ Comprehensive analysis and reporting\")\n",
    "print(\"  ✓ Integration patterns for external systems\")\n",
    "print(\"\\n🚀 You're ready to build advanced thesis chat applications!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",
n",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}